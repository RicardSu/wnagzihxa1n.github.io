<!DOCTYPE html>
<html>
<head>
<title>利用爬虫爬取煎蛋上面的小黄图</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">
/* GitHub stylesheet for MarkdownPad (http://markdownpad.com) */
/* Author: Nicolas Hery - http://nicolashery.com */
/* Version: b13fe65ca28d2e568c6ed5d7f06581183df8f2ff */
/* Source: https://github.com/nicolahery/markdownpad-github */

/* RESET
=============================================================================*/

html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, img, ins, kbd, q, s, samp, small, strike, strong, sub, sup, tt, var, b, u, i, center, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td, article, aside, canvas, details, embed, figure, figcaption, footer, header, hgroup, menu, nav, output, ruby, section, summary, time, mark, audio, video {
  margin: 0;
  padding: 0;
  border: 0;
}

/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

body>*:first-child {
  margin-top: 0 !important;
}

body>*:last-child {
  margin-bottom: 0 !important;
}

/* BLOCKS
=============================================================================*/

p, blockquote, ul, ol, dl, table, pre {
  margin: 15px 0;
}

/* HEADERS
=============================================================================*/

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
}

h1 tt, h1 code, h2 tt, h2 code, h3 tt, h3 code, h4 tt, h4 code, h5 tt, h5 code, h6 tt, h6 code {
  font-size: inherit;
}

h1 {
  font-size: 28px;
  color: #000;
}

h2 {
  font-size: 24px;
  border-bottom: 1px solid #ccc;
  color: #000;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777;
  font-size: 14px;
}

body>h2:first-child, body>h1:first-child, body>h1:first-child+h2, body>h3:first-child, body>h4:first-child, body>h5:first-child, body>h6:first-child {
  margin-top: 0;
  padding-top: 0;
}

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0;
}

h1+p, h2+p, h3+p, h4+p, h5+p, h6+p {
  margin-top: 10px;
}

/* LINKS
=============================================================================*/

a {
  color: #4183C4;
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

/* LISTS
=============================================================================*/

ul, ol {
  padding-left: 30px;
}

ul li > :first-child, 
ol li > :first-child, 
ul li ul:first-of-type, 
ol li ol:first-of-type, 
ul li ol:first-of-type, 
ol li ul:first-of-type {
  margin-top: 0px;
}

ul ul, ul ol, ol ol, ol ul {
  margin-bottom: 0;
}

dl {
  padding: 0;
}

dl dt {
  font-size: 14px;
  font-weight: bold;
  font-style: italic;
  padding: 0;
  margin: 15px 0 5px;
}

dl dt:first-child {
  padding: 0;
}

dl dt>:first-child {
  margin-top: 0px;
}

dl dt>:last-child {
  margin-bottom: 0px;
}

dl dd {
  margin: 0 0 15px;
  padding: 0 15px;
}

dl dd>:first-child {
  margin-top: 0px;
}

dl dd>:last-child {
  margin-bottom: 0px;
}

/* CODE
=============================================================================*/

pre, code, tt {
  font-size: 12px;
  font-family: Consolas, "Liberation Mono", Courier, monospace;
}

code, tt {
  margin: 0 0px;
  padding: 0px 0px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px;
}

pre>code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent;
}

pre {
  background-color: #f8f8f8;
  border: 1px solid #ccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px;
}

pre code, pre tt {
  background-color: transparent;
  border: none;
}

kbd {
    -moz-border-bottom-colors: none;
    -moz-border-left-colors: none;
    -moz-border-right-colors: none;
    -moz-border-top-colors: none;
    background-color: #DDDDDD;
    background-image: linear-gradient(#F1F1F1, #DDDDDD);
    background-repeat: repeat-x;
    border-color: #DDDDDD #CCCCCC #CCCCCC #DDDDDD;
    border-image: none;
    border-radius: 2px 2px 2px 2px;
    border-style: solid;
    border-width: 1px;
    font-family: "Helvetica Neue",Helvetica,Arial,sans-serif;
    line-height: 10px;
    padding: 1px 4px;
}

/* QUOTES
=============================================================================*/

blockquote {
  border-left: 4px solid #DDD;
  padding: 0 15px;
  color: #777;
}

blockquote>:first-child {
  margin-top: 0px;
}

blockquote>:last-child {
  margin-bottom: 0px;
}

/* HORIZONTAL RULES
=============================================================================*/

hr {
  clear: both;
  margin: 15px 0;
  height: 0px;
  overflow: hidden;
  border: none;
  background: transparent;
  border-bottom: 4px solid #ddd;
  padding: 0;
}

/* TABLES
=============================================================================*/

table th {
  font-weight: bold;
}

table th, table td {
  border: 1px solid #ccc;
  padding: 6px 13px;
}

table tr {
  border-top: 1px solid #ccc;
  background-color: #fff;
}

table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

/* IMAGES
=============================================================================*/

img {
  max-width: 100%
}
</style>
<style type="text/css">
.highlight  { background: #ffffff; }
.highlight .c { color: #999988; font-style: italic } /* Comment */
.highlight .err { color: #a61717; background-color: #e3d2d2 } /* Error */
.highlight .k { font-weight: bold } /* Keyword */
.highlight .o { font-weight: bold } /* Operator */
.highlight .cm { color: #999988; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #999999; font-weight: bold } /* Comment.Preproc */
.highlight .c1 { color: #999988; font-style: italic } /* Comment.Single */
.highlight .cs { color: #999999; font-weight: bold; font-style: italic } /* Comment.Special */
.highlight .gd { color: #000000; background-color: #ffdddd } /* Generic.Deleted */
.highlight .gd .x { color: #000000; background-color: #ffaaaa } /* Generic.Deleted.Specific */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #aa0000 } /* Generic.Error */
.highlight .gh { color: #999999 } /* Generic.Heading */
.highlight .gi { color: #000000; background-color: #ddffdd } /* Generic.Inserted */
.highlight .gi .x { color: #000000; background-color: #aaffaa } /* Generic.Inserted.Specific */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #555555 } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #aaaaaa } /* Generic.Subheading */
.highlight .gt { color: #aa0000 } /* Generic.Traceback */
.highlight .kc { font-weight: bold } /* Keyword.Constant */
.highlight .kd { font-weight: bold } /* Keyword.Declaration */
.highlight .kp { font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #445588; font-weight: bold } /* Keyword.Type */
.highlight .m { color: #009999 } /* Literal.Number */
.highlight .s { color: #d14 } /* Literal.String */
.highlight .na { color: #008080 } /* Name.Attribute */
.highlight .nb { color: #0086B3 } /* Name.Builtin */
.highlight .nc { color: #445588; font-weight: bold } /* Name.Class */
.highlight .no { color: #008080 } /* Name.Constant */
.highlight .ni { color: #800080 } /* Name.Entity */
.highlight .ne { color: #990000; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #990000; font-weight: bold } /* Name.Function */
.highlight .nn { color: #555555 } /* Name.Namespace */
.highlight .nt { color: #000080 } /* Name.Tag */
.highlight .nv { color: #008080 } /* Name.Variable */
.highlight .ow { font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mf { color: #009999 } /* Literal.Number.Float */
.highlight .mh { color: #009999 } /* Literal.Number.Hex */
.highlight .mi { color: #009999 } /* Literal.Number.Integer */
.highlight .mo { color: #009999 } /* Literal.Number.Oct */
.highlight .sb { color: #d14 } /* Literal.String.Backtick */
.highlight .sc { color: #d14 } /* Literal.String.Char */
.highlight .sd { color: #d14 } /* Literal.String.Doc */
.highlight .s2 { color: #d14 } /* Literal.String.Double */
.highlight .se { color: #d14 } /* Literal.String.Escape */
.highlight .sh { color: #d14 } /* Literal.String.Heredoc */
.highlight .si { color: #d14 } /* Literal.String.Interpol */
.highlight .sx { color: #d14 } /* Literal.String.Other */
.highlight .sr { color: #009926 } /* Literal.String.Regex */
.highlight .s1 { color: #d14 } /* Literal.String.Single */
.highlight .ss { color: #990073 } /* Literal.String.Symbol */
.highlight .bp { color: #999999 } /* Name.Builtin.Pseudo */
.highlight .vc { color: #008080 } /* Name.Variable.Class */
.highlight .vg { color: #008080 } /* Name.Variable.Global */
.highlight .vi { color: #008080 } /* Name.Variable.Instance */
.highlight .il { color: #009999 } /* Literal.Number.Integer.Long */
.pl-c {
    color: #969896;
}

.pl-c1,.pl-mdh,.pl-mm,.pl-mp,.pl-mr,.pl-s1 .pl-v,.pl-s3,.pl-sc,.pl-sv {
    color: #0086b3;
}

.pl-e,.pl-en {
    color: #795da3;
}

.pl-s1 .pl-s2,.pl-smi,.pl-smp,.pl-stj,.pl-vo,.pl-vpf {
    color: #333;
}

.pl-ent {
    color: #63a35c;
}

.pl-k,.pl-s,.pl-st {
    color: #a71d5d;
}

.pl-pds,.pl-s1,.pl-s1 .pl-pse .pl-s2,.pl-sr,.pl-sr .pl-cce,.pl-sr .pl-sra,.pl-sr .pl-sre,.pl-src,.pl-v {
    color: #df5000;
}

.pl-id {
    color: #b52a1d;
}

.pl-ii {
    background-color: #b52a1d;
    color: #f8f8f8;
}

.pl-sr .pl-cce {
    color: #63a35c;
    font-weight: bold;
}

.pl-ml {
    color: #693a17;
}

.pl-mh,.pl-mh .pl-en,.pl-ms {
    color: #1d3e81;
    font-weight: bold;
}

.pl-mq {
    color: #008080;
}

.pl-mi {
    color: #333;
    font-style: italic;
}

.pl-mb {
    color: #333;
    font-weight: bold;
}

.pl-md,.pl-mdhf {
    background-color: #ffecec;
    color: #bd2c00;
}

.pl-mdht,.pl-mi1 {
    background-color: #eaffea;
    color: #55a532;
}

.pl-mdr {
    color: #795da3;
    font-weight: bold;
}

.pl-mo {
    color: #1d3e81;
}
.task-list {
padding-left:10px;
margin-bottom:0;
}

.task-list li {
    margin-left: 20px;
}

.task-list-item {
list-style-type:none;
padding-left:10px;
}

.task-list-item label {
font-weight:400;
}

.task-list-item.enabled label {
cursor:pointer;
}

.task-list-item+.task-list-item {
margin-top:3px;
}

.task-list-item-checkbox {
display:inline-block;
margin-left:-20px;
margin-right:3px;
vertical-align:1px;
}
</style>
</head>
<body>
<h1 id="-">利用爬虫爬取煎蛋上面的小黄图</h1>
<p><strong>Author：wnagzihxa1n<br>Mail：tudouboom@163.com</strong></p>
<h2 id="0x00-">0x00 前言</h2>
<p>ichunqiu的一个Python课程里面提到了爬煎蛋上面的小黄图，我这里也玩一玩</p>
<ul>
<li>ADO-Python安全工具开发应用：<a href="http://www.ichunqiu.com/course/53437">http://www.ichunqiu.com/course/53437</a><ul>
<li>3.4 爬虫多线程</li><li>3.5 爬虫正则表达式</li></ul>
</li></ul>
<p>需要购买，大家可以去支持一下</p>
<h2 id="0x01-">0x01 准备工作</h2>
<ul>
<li>煎蛋：<a href="http://jandan.net/">http://jandan.net/</a></li></ul>
<p>我们点击<strong>妹子图</strong></p>
<p><img src="Image/1.png" alt=""></p>
<p>通过点击页数，我们可以发现URL的变化以及规律</p>
<pre><code>http://jandan.net/ooxx/page-2365#comments
http://jandan.net/ooxx/page-2364#comments
http://jandan.net/ooxx/page-2363#comments
http://jandan.net/ooxx/page-2362#comments
</code></pre><p>总结出URL的规律：</p>
<pre><code>http://jandan.net/ooxx/page-PageNumber#comments
</code></pre><p>再可以发现，有图片，还有GIF，这点在后面的获取要注意</p>
<h2 id="0x02-">0x02 动手实践</h2>
<p>根据我们前一篇(<a href="http://www.wangzhixian.org/PythonSpider/ichunqiu%20AllCourses%20Info%20Spider/article.html">ichunqiu AllCourses Info Spider</a>)的知识</p>
<p>Burp抓包，找到请求的方式，最近1.7出来了，蛮好用的推荐一下</p>
<pre><code>GET /ooxx/page-2363 HTTP/1.1
Host: jandan.net
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:50.0) Gecko/20100101 Firefox/50.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Language: zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3
Accept-Encoding: gzip, deflate
Connection: close
Upgrade-Insecure-Requests: 1
</code></pre><p>我们找到请求包后，发现是GET的方式，那么我们就可以写程序来模拟请求该页面了</p>
<pre><code>#coding:utf-8

import requests

header = {    
        &#39;GET&#39;: &#39;/ooxx/page-2363 HTTP/1.1&#39;,
        &#39;Host&#39;: &#39;jandan.net&#39;,
        &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:50.0) Gecko/20100101 Firefox/50.0&#39;,
        &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,
        &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3&#39;,
        &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;,
        &#39;Cookie&#39;: &#39;jdna=01b0531fab6a989460dd1b231010b496#1487864130124; _ga=GA1.2.78402845.1487864133; _gat=1&#39;,
        &#39;Connection&#39;: &#39;close&#39;,
        &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;,
    }

def main():
    html = requests.get(url = &#39;http://jandan.net/ooxx/page-2363#comments&#39;, headers = header)
    print html.text

if __name__ == &#39;__main__&#39;:
    main()
</code></pre><p>跑起来</p>
<p><img src="Image/2.png" alt=""></p>
<p>接下来我们就可以尝试遍历所有页面了，在此之前，我们去掉header的第一个item</p>
<pre><code class="lang-`">&#39;GET&#39;: &#39;/ooxx/page-2363 HTTP/1.1&#39;,
</code></pre>
<p>遍历所有页面就是写个循环遍历URL中的PageNumber，这里先遍历100个页面</p>
<pre><code>#coding:utf-8

import requests

header = {    
        &#39;Host&#39;: &#39;jandan.net&#39;,
        &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:50.0) Gecko/20100101 Firefox/50.0&#39;,
        &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,
        &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3&#39;,
        &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;,
        &#39;Cookie&#39;: &#39;jdna=01b0531fab6a989460dd1b231010b496#1487864130124; _ga=GA1.2.78402845.1487864133; _gat=1&#39;,
        &#39;Connection&#39;: &#39;close&#39;,
        &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;,
    }

def main():
    for i in range(2000, 2100):
        url = &#39;http://jandan.net/ooxx/page-&#39; + str(i) + &#39;#comments&#39;
        html = requests.get(url = url, headers = header)
        print html.status_code

if __name__ == &#39;__main__&#39;:
    main()
</code></pre><p>总时长</p>
<pre><code>[Finished in 56.5s]
</code></pre><p>才一百个页面就要接近1分钟的时间，这谁受得了，我们使用多线程来加快速度，将访问的代码段封装成<code>spider(url)</code>函数</p>
<pre><code>#coding:utf-8

import requests
import threading

header = {    
        &#39;Host&#39;: &#39;jandan.net&#39;,
        &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:50.0) Gecko/20100101 Firefox/50.0&#39;,
        &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,
        &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3&#39;,
        &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;,
        &#39;Cookie&#39;: &#39;jdna=01b0531fab6a989460dd1b231010b496#1487864130124; _ga=GA1.2.78402845.1487864133; _gat=1&#39;,
        &#39;Connection&#39;: &#39;close&#39;,
        &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;,
    }

def spider(url):
    html = requests.get(url = url, headers = header)
    print html.status_code

def main():
    urls = []
    for i in range(2000, 2100):
        url = &#39;http://jandan.net/ooxx/page-&#39; + str(i) + &#39;#comments&#39;
        urls.append(url)

    threads = []
    threads_count = len(urls)

    for i in range(threads_count):
        t = threading.Thread(target = spider, args = (urls[i],))
        threads.append(t)

    for i in range(threads_count):
        threads[i].start()

    for i in range(threads_count):
        threads[i].join()

if __name__ == &#39;__main__&#39;:
    main()
</code></pre><p>总时长</p>
<pre><code>[Finished in 1.4s]
</code></pre><p>这就好多了</p>
<p>我们再来优化，使用队列<code>Queue</code></p>
<pre><code>#coding:utf-8

import requests
import threading
import Queue

header = {    
        &#39;Host&#39;: &#39;jandan.net&#39;,
        &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:50.0) Gecko/20100101 Firefox/50.0&#39;,
        &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,
        &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3&#39;,
        &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;,
        &#39;Cookie&#39;: &#39;jdna=01b0531fab6a989460dd1b231010b496#1487864130124; _ga=GA1.2.78402845.1487864133; _gat=1&#39;,
        &#39;Connection&#39;: &#39;close&#39;,
        &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;,
    }

class JiandanSpider(threading.Thread):
    &quot;&quot;&quot;docstring for JiandanSpider&quot;&quot;&quot;
    def __init__(self, queue):
        super(JiandanSpider, self).__init__()
        self.queue = queue

    def run(self):
        while not self.queue.empty():
            url = self.queue.get_nowait()
            self.spider(url)
            pass

    def spider(self, url):
        html = requests.get(url = url, headers = header)
        print html.status_code

def main():
    queue = Queue.Queue()

    for i in range(2000, 2100):
        url = &#39;http://jandan.net/ooxx/page-&#39; + str(i) + &#39;#comments&#39;
        queue.put(url)

    threads = []
    threads_count = 10

    for i in range(threads_count):
        threads.append(JiandanSpider(queue))

    for thread in threads:
        thread.start()

    for thread in threads:
        thread.join()

if __name__ == &#39;__main__&#39;:
    main()
</code></pre><p>总时长</p>
<pre><code>[Finished in 7.8s]
</code></pre><p>接着我们就要开始做关键的工作了，找到代码里的图片链接，这里一定要根据自己当时获取到的源码来分析，毕竟人家是会升级的</p>
<p>图片的链接，我们发现都是jpg格式的</p>
<pre><code>&lt;img src=&quot;//wx1.sinaimg.cn/mw600/006GlaT2ly1fd0orhmsiaj30m90xctdl.jpg&quot; /&gt;&lt;/p&gt;
</code></pre><p>所以用正则表达式来提取</p>
<pre><code>re.findall(&#39;&lt;img src=&quot;(.*?)&quot; /&gt;&lt;/p&gt;&#39;, html.content)
</code></pre><p>我们先测试一下，将线程数修改为1，页面数量也修改为1</p>
<pre><code>#coding:utf-8

import requests
import threading
import Queue
import re

header = {    
        &#39;Host&#39;: &#39;jandan.net&#39;,
        &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:50.0) Gecko/20100101 Firefox/50.0&#39;,
        &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,
        &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3&#39;,
        &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;,
        &#39;Cookie&#39;: &#39;jdna=01b0531fab6a989460dd1b231010b496#1487864130124; _ga=GA1.2.78402845.1487864133; _gat=1&#39;,
        &#39;Connection&#39;: &#39;close&#39;,
        &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;,
    }

class JiandanSpider(threading.Thread):
    &quot;&quot;&quot;docstring for JiandanSpider&quot;&quot;&quot;
    def __init__(self, queue):
        super(JiandanSpider, self).__init__()
        self.queue = queue

    def run(self):
        while not self.queue.empty():
            url = self.queue.get_nowait()
            self.spider(url)
            pass

    def spider(self, url):
        html = requests.get(url = url, headers = header)
        imgs = re.findall(&#39;&lt;img src=&quot;(.*?)&quot; /&gt;&lt;/p&gt;&#39;, html.content)
        for img in imgs:
            print img

def main():
    queue = Queue.Queue()

    for i in range(2000, 2001):
        url = &#39;http://jandan.net/ooxx/page-&#39; + str(i) + &#39;#comments&#39;
        queue.put(url)

    threads = []
    threads_count = 1

    for i in range(threads_count):
        threads.append(JiandanSpider(queue))

    for thread in threads:
        thread.start()

    for thread in threads:
        thread.join()

if __name__ == &#39;__main__&#39;:
    main()
</code></pre><p>运行部分结果输出</p>
<pre><code>//ww2.sinaimg.cn/mw600/6cca1403jw1f7975dqxq3j20dh0hodgp.jpg
//ww2.sinaimg.cn/mw600/83f596c9gw1f796ylchtoj20ia0pu10z.jpg
//ww2.sinaimg.cn/mw600/83f596c9gw1f796yltquyj20ia0rgdiy.jpg
//ww2.sinaimg.cn/mw600/dab1a860jw1f78q2spihjj20zk0qoah3.jpg
//ww3.sinaimg.cn/mw600/005vbOHfgw1f78l1v9sgxj30h40omjux.jpg
//ww3.sinaimg.cn/mw600/005vbOHfgw1f78l2hmmg4j30e20p0ab5.jpg
//ww4.sinaimg.cn/mw600/5ff46675gw1f78i2vrpq3j20k00qomzp.jpg
//ww4.sinaimg.cn/mw600/5ff46675gw1f78i2wbbtyj20k00qoab0.jpg
//ww2.sinaimg.cn/mw600/5ff46675gw1f78i2wj8rjj20ku0rswfm.jpg
//ww2.sinaimg.cn/mw600/6469180ajw1f78bfo5kozj20ku0usn1p.jpg
//ww3.sinaimg.cn/mw600/6469180ajw1f78fkcimr6j20bl0dbq4p.jpg
//ww4.sinaimg.cn/mw600/6469180ajw1f78fpuk8oaj20dw0kujt8.jpg
//ww1.sinaimg.cn/mw600/6469180ajw1f78g4ro6kfj20b10go3zn.jpg
//ww1.sinaimg.cn/mw600/a00dfa2agw1f78fa9rnnjj212w0lw77v.jpg
[Finished in 3.4s]
</code></pre><p>随便找一个url访问一下，太靓我就不放截图了</p>
<p>处理了jpg之后我们来处理一下GIF的链接提取，但是我们发现它有两个链接</p>
<pre><code>&lt;img src=&quot;//wx3.sinaimg.cn/thumb180/006mbEeOgy1fd0jlwosung30cg070b2a.gif&quot; 
    org_src=&quot;//wx3.sinaimg.cn/mw1024/006mbEeOgy1fd0jlwosung30cg070b2a.gif&quot; 
    onload=&quot;add_img_loading_mask(this, load_sina_gif);&quot;/&gt;&lt;/p&gt;
</code></pre><p>通过访问，我们发现<code>src</code>后面的链接图片是静态的，而<code>org_src</code>后面的图片是动态的</p>
<p>所以我们用下面的正则表达式来提取</p>
<pre><code>re.findall(&#39;org_src=&quot;(.*?)&quot; &#39;, html.content)
</code></pre><p>我们恢复线程数为10，因为GIF链接相对较少，我们将页面数量设为10</p>
<pre><code>#coding:utf-8

import requests
import threading
import Queue
import re

header = {    
        &#39;Host&#39;: &#39;jandan.net&#39;,
        &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:50.0) Gecko/20100101 Firefox/50.0&#39;,
        &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,
        &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3&#39;,
        &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;,
        &#39;Cookie&#39;: &#39;jdna=01b0531fab6a989460dd1b231010b496#1487864130124; _ga=GA1.2.78402845.1487864133; _gat=1&#39;,
        &#39;Connection&#39;: &#39;close&#39;,
        &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;,
    }

class JiandanSpider(threading.Thread):
    &quot;&quot;&quot;docstring for JiandanSpider&quot;&quot;&quot;
    def __init__(self, queue):
        super(JiandanSpider, self).__init__()
        self.queue = queue

    def run(self):
        while not self.queue.empty():
            url = self.queue.get_nowait()
            self.spider(url)

    def spider(self, url):
        html = requests.get(url = url, headers = header)
        imgs = re.findall(&#39;&lt;img src=&quot;(.*?)&quot; /&gt;&lt;/p&gt;&#39;, html.content)
        gifs = re.findall(&#39;org_src=&quot;(.*?)&quot; &#39;, html.content)
        # for img in imgs:
        #     print img
        for gif in gifs:
            print gif

def main():
    queue = Queue.Queue()

    for i in range(2000, 2010):
        url = &#39;http://jandan.net/ooxx/page-&#39; + str(i) + &#39;#comments&#39;
        queue.put(url)

    threads = []
    threads_count = 10

    for i in range(threads_count):
        threads.append(JiandanSpider(queue))

    for thread in threads:
        thread.start()

    for thread in threads:
        thread.join()

if __name__ == &#39;__main__&#39;:
    main()
</code></pre><p>运行部分结果输出</p>
<pre><code>//ww3.sinaimg.cn/mw1024/005vbOHfgw1f4bbudodj3g307n0fax6p.gif
//ww2.sinaimg.cn/mw690/005G9wwngw1f4b1vzn8pqg30ak0fs1l0.gif
//ww2.sinaimg.cn/mw690/c1a3c815gw1f48ny33al9g208c04p4gi.gif
//ww3.sinaimg.cn/mw1024/005vbOHfgw1f4ds1uhcaxg30dw0cix6s.gif
//ww2.sinaimg.cn/mw1024/005vbOHfgw1f4i7r36jdsg308c08c4qr.gif
//ww1.sinaimg.cn/mw1024/005vbOHfgw1f4i7rfcw8ag308c08c4qr.gif
//ww4.sinaimg.cn/mw690/9ec19de8gw1eshy0wnjnag20b4069he1.gif
//ww2.sinaimg.cn/mw690/444d7111jw1f4lcxzdlu1g20dw0a7b2p.gif
//ww3.sinaimg.cn/mw690/9bd522c1gw1f4kn2n4h80g20a009wu0z.gif
[Finished in 0.6s]
</code></pre><p>GIF比jpg还黄，跳过截图这个步骤</p>
<p>最后一步，根据获取到的URL下载图片或者GIF</p>
<p>我们使用urllib库的<code>urlretrieve()</code>方法，文件命名使用原名称，但是有一个地方要注意，在前面加上<code>http:</code>，并使用递归的方法来减少因为网络环境问题导致的下载错误</p>
<pre><code>#coding:utf-8

import requests
import threading
import Queue
import re
import urllib
import time

header = {    
        &#39;Host&#39;: &#39;jandan.net&#39;,
        &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:50.0) Gecko/20100101 Firefox/50.0&#39;,
        &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,
        &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3&#39;,
        &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;,
        &#39;Connection&#39;: &#39;close&#39;,
        &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;,
    }

class JiandanSpider(threading.Thread):
    &quot;&quot;&quot;docstring for JiandanSpider&quot;&quot;&quot;
    def __init__(self, queue):
        super(JiandanSpider, self).__init__()
        self.queue = queue

    def run(self):
        while not self.queue.empty():
            url = self.queue.get_nowait()
            self.spider(url)

    def spider(self, url):
        html = requests.get(url = url, headers = header)
        imgs = re.findall(&#39;&lt;img src=&quot;(.*?)&quot; /&gt;&lt;/p&gt;&#39;, html.content)
        gifs = re.findall(&#39;org_src=&quot;(.*?)&quot; &#39;, html.content)
        for img in imgs:
            img_name = img.split(&#39;/&#39;)[-1]
            self.dump_img(img, img_name)
        for gif in gifs:
            gif_name = gif.split(&#39;/&#39;)[-1]
            self.dump_img(gif, gif_name)

    def dump_img(self, url, img_name):
        try:
            urllib.urlretrieve(&#39;http:&#39; + url, filename = &#39;img/&#39; + img_name)
        except urllib.ContentTooShortError:
            raise e
            dump_img(self, url, img_name)

def main():
    queue = Queue.Queue()

    for i in range(2000, 2010):
        url = &#39;http://jandan.net/ooxx/page-&#39; + str(i) + &#39;#comments&#39;
        queue.put(url)

    threads = []
    threads_count = 5

    for i in range(threads_count):
        threads.append(JiandanSpider(queue))

    for thread in threads:
        thread.start()

    for thread in threads:
        thread.join()

if __name__ == &#39;__main__&#39;:
    urllib.urlretrieve(&#39;http://wx1.sinaimg.cn/mw600/006GlaT2ly1fd0orhmsiaj30m90xctdl.jpg&#39;, filename = &#39;1.jpg&#39;)
    main()
</code></pre><p>跑起来，发现不妙，所有图片都是1kb，明显是出错了</p>
<p><img src="Image/3.png" alt=""></p>
<p>使用16进制编辑器查看文件内容</p>
<pre><code>&lt;html&gt;&lt;body&gt;&lt;h1&gt;403 Forbidden&lt;/h1&gt;
Request forbidden by administrative rules.
&lt;/body&gt;&lt;/html&gt;
</code></pre><p>各种网上搜资料，加上睡眠时间等，最终抛弃<code>urllib.urlretrieve()</code>，换了一种新方法</p>
<pre><code>#coding:utf-8

import requests
import threading
import Queue
import re
import urllib
import time
import shutil

header = {    
        &#39;Host&#39;: &#39;jandan.net&#39;,
        &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:50.0) Gecko/20100101 Firefox/50.0&#39;,
        &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,
        &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3&#39;,
        &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;,
        &#39;Connection&#39;: &#39;close&#39;,
        &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;,
    }

class JiandanSpider(threading.Thread):
    &quot;&quot;&quot;docstring for JiandanSpider&quot;&quot;&quot;
    def __init__(self, queue):
        super(JiandanSpider, self).__init__()
        self.queue = queue

    def run(self):
        while not self.queue.empty():
            url = self.queue.get_nowait()
            self.spider(url)

    def spider(self, url):
        html = requests.get(url = url, headers = header)
        imgs = re.findall(&#39;&lt;img src=&quot;(.*?)&quot; /&gt;&lt;/p&gt;&#39;, html.content)
        gifs = re.findall(&#39;org_src=&quot;(.*?)&quot; &#39;, html.content)
        for img in imgs:
            img_name = img.split(&#39;/&#39;)[-1]
            # print &#39;Dumping &#39; + img_name
            self.dump_img(img, img_name)
            # time.sleep(1)
        for gif in gifs:
            gif_name = gif.split(&#39;/&#39;)[-1]
            self.dump_img(gif, gif_name)

    def dump_img(self, url, img_name):
        try:
            img_stream = requests.get(url = &#39;http:&#39; + url, stream = True)
            if img_stream.status_code == 200:
                with open(&#39;img/&#39; + img_name, &#39;wb&#39;) as f:
                    img_stream.raw.decode_content = True
                    shutil.copyfileobj(img_stream.raw, f)
        except urllib.ContentTooShortError:
            raise e
            dump_img(self, url, img_name)

def main():
    queue = Queue.Queue()

    for i in range(2000, 2010):
        url = &#39;http://jandan.net/ooxx/page-&#39; + str(i) + &#39;#comments&#39;
        queue.put(url)

    threads = []
    threads_count = 5

    for i in range(threads_count):
        threads.append(JiandanSpider(queue))

    for thread in threads:
        thread.start()

    for thread in threads:
        thread.join()

if __name__ == &#39;__main__&#39;:
    urllib.urlretrieve(&#39;http://wx1.sinaimg.cn/mw600/006GlaT2ly1fd0orhmsiaj30m90xctdl.jpg&#39;, filename = &#39;1.jpg&#39;)
    main()
</code></pre><p>跑起来，蛮好的</p>
<p><img src="Image/4.png" alt=""></p>
<p>然后把线程数调大一点，把页数调整好，丢那跑就好了</p>
<p>但是真的完了吗？</p>
<p>我们调整页数后发现了至少两个问题</p>
<ul>
<li>有的URL是<code>http://</code>开头，有的是<code>//</code>开头</li><li>有的URL已经无法访问</li><li>网速问题</li></ul>
<p>针对这几个问题，我们设置<code>timeout</code>参数，设置黑名单检测，同时捕获异常</p>
<pre><code>#coding:utf-8

import requests
import threading
import Queue
import re
import urllib
import time
import shutil

header = {    
        &#39;Host&#39;: &#39;jandan.net&#39;,
        &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:50.0) Gecko/20100101 Firefox/50.0&#39;,
        &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;,
        &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3&#39;,
        &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;,
        &#39;Connection&#39;: &#39;close&#39;,
        &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;,
    }

Bad_urls = []

class JiandanSpider(threading.Thread):
    &quot;&quot;&quot;docstring for JiandanSpider&quot;&quot;&quot;
    def __init__(self, queue):
        super(JiandanSpider, self).__init__()
        self.queue = queue

    def run(self):
        while not self.queue.empty():
            url = self.queue.get_nowait()
            self.spider(url)

    def spider(self, url):
        # print url
        html = requests.get(url = url, headers = header)
        imgs = re.findall(&#39;&lt;img src=&quot;(.*?)&quot; /&gt;&lt;/p&gt;&#39;, html.content)
        gifs = re.findall(&#39;org_src=&quot;(.*?)&quot; &#39;, html.content)
        for img in imgs:
            img_name = img.split(&#39;/&#39;)[-1]
            self.dump_img(img, img_name)
        for gif in gifs:
            gif_name = gif.split(&#39;/&#39;)[-1]
            self.dump_img(gif, gif_name)

    def dump_img(self, url, img_name):
        try:
            if len(url) == 0:
                return
            if self.checkURLReachable(url) == True:
                return
            if url[0] != &#39;h&#39;:
                url = &#39;http:&#39; + url
            try:
                img_stream = requests.get(url = url.strip(), stream = True, timeout = 5, allow_redirects = False)
                if img_stream.status_code == 200:
                    with open(&#39;img/&#39; + img_name, &#39;wb&#39;) as f:
                        img_stream.raw.decode_content = True
                        shutil.copyfileobj(img_stream.raw, f)
            except Exception as e:
                print &#39;\&#39;&#39; + url + &#39;\&#39;,&#39;
        except urllib.ContentTooShortError:
            dump_img(url, img_name)

    def checkURLReachable(self, url):
        Black_List = [&#39;farm3.static.flickr.com&#39;, &#39;farm4.static.flickr.com&#39;, &#39;farm5.static.flickr.com&#39;, &#39;farm6.static.flickr.com&#39;, &#39;farm7.static.flickr.com&#39;, &#39;k.min.us&#39;, &#39;jmdou.com&#39;, &#39;i.min.us&#39;, 
                        &#39;www.tumblr.com&#39;, &#39;24.media.tumblr.com&#39;, &#39;25.media.tumblr.com&#39;, &#39;26.media.tumblr.com&#39;, &#39;27.media.tumblr.com&#39;, &#39;28.media.tumblr.com&#39;, &#39;29.media.tumblr.com&#39;, &#39;30.media.tumblr.com&#39;, 
                        &#39;img.ffffound.com&#39;, &#39;static.lisi.com.cn&#39;, &#39;farm8.staticflickr.com&#39;, &#39;farm7.staticflickr.com&#39;, &#39;a8.sphotos.ak.fbcdn.net&#39;, &#39;cdnimg.visualizeus.com&#39;, &#39;103.imagebam.com&#39;,
                        &#39;data.tumblr.com&#39;, &#39;www2.ff369.com&#39;, &#39;m2.img.libdd.com&#39;, &#39;m1.img.libdd.com&#39;, &#39;m3.img.libdd.com&#39;, &#39;i1201.photobucket.com&#39;, &#39;fs1.clubzone.cn&#39;,&#39;105.imagebam.com&#39;, &#39;102.imagebam.com&#39;, 
                        &#39;106.imagebam.com&#39;, &#39;101.imagebam.com&#39;, ]
        url = url.split(&#39;/&#39;)[2]
        if url in Black_List:
            return True
        return False


def main():
    queue = Queue.Queue()

    for i in range(1, 2365):
        url = &#39;http://jandan.net/ooxx/page-&#39; + str(i) + &#39;#comments&#39;
        queue.put(url)

    threads = []
    threads_count = 50

    for i in range(threads_count):
        threads.append(JiandanSpider(queue))

    for thread in threads:
        thread.start()

    for thread in threads:
        thread.join()

if __name__ == &#39;__main__&#39;:
    pass
    # main()
</code></pre><p>关于黑名单，可以先将<code>timeout</code>设的小一点先跑一遍，然后将异常的URL输出，判断服务器是否处于正常运行状态，上面我只是将部分已经不能访问的域名列出来</p>
<p>那么这里还有一个问题要解决：</p>
<ul>
<li>捕获到异常的URL，如何去重新遍历？</li></ul>
<p>既然捕获到异常，说明一定不是在黑名单里，也就是肯定是可以访问的，我们将<code>timeout</code>设置的大一点，因为网络等原因可能会出现卡顿，循环就行</p>
<p>不过最后测试的时候还是有一些问题，如果有同学碰到我这里没有提到的问题，可以自己解决一下</p>
<h2 id="0x03-">0x03 小结</h2>
<p>搞爬虫这种东西，慢慢积累经验吧</p>
<h2 id="0x04-references">0x04 References</h2>
<ul>
<li><a href="http://stackoverflow.com/questions/34692009/download-image-from-url-using-python-urllib-but-receiving-http-error-403-forbid">http://stackoverflow.com/questions/34692009/download-image-from-url-using-python-urllib-but-receiving-http-error-403-forbid</a></li><li><a href="http://stackoverflow.com/questions/24226781/changing-user-agent-in-python-3-for-urrlib-request-urlopen">http://stackoverflow.com/questions/24226781/changing-user-agent-in-python-3-for-urrlib-request-urlopen</a></li></ul>

</body>
</html>
<!-- This document was created with MarkdownPad, the Markdown editor for Windows (http://markdownpad.com) -->
